from typing import Dict, Tuple

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler


def load_mhrv_xls(
        path: str, sheet_names: list
) -> Dict[str, pd.DataFrame]:
    """
    Loads raw data from excel file containing HRV features generated by mhrv.
    @param path: Path to file.
    @param sheet_names: Names of excel sheets to load.
    @return: A dict from sheet name to pandas dataframe. The summary
    statistics rows will be removed.
    """

    dfs: Dict[str, pd.DataFrame] = pd.read_excel(
        path, header=0, index_col=0,
        sheet_name=list(sheet_names),
    )

    summary_rows = ('Mean', 'Median', 'SE')
    for name, df in dfs.items():
        # Remove the rows which contain summary statistics
        df = df.loc[~df.index.isin(summary_rows), :]

        # Convert dtypes
        df = df.astype(np.float32, copy=False)
        df = df.astype(dict(RR=np.int32, NN=np.int32), copy=False)

        # Impute NaN with mean
        df.fillna(value=df.mean(skipna=True), inplace=True)

        dfs[name] = df

        print(f'Loaded {name}: {len(df)} samples, {len(df.columns)} features')

    return dfs


def create_ci_dataset(
        df_control: pd.DataFrame, df_treated: pd.DataFrame,
        psd_type: str = 'AR',
) -> pd.DataFrame:
    """
    Creates the dataset for this project as a single dataframe.
    @param df_control: DataFrame with control data.
    @param df_treated: DataFrame with post-treatment data.
    @param psd_type: Type of PSD estimate to take from dataset,
    which contains multiple estimates as e.g. "VLF_POWER_WELCH". The
    selected variables will be renamed to e.g. "VLF_POWER".
    @return: Single dataframe with consolidated PSD features and a treatment
    column 'T'.
    """

    # Treat PSD columns: Keep only the requested type, remove the other type
    # and rename the columns.
    psd_suffix = f'_{psd_type.upper()}'

    def consolidate_psd(df: pd.DataFrame):
        # Discover PSD columns
        cols = df.columns
        psd_cols = set(
            map(lambda c: c.split(psd_suffix)[0],
                filter(lambda c: c.endswith(psd_suffix), cols))
        )

        # Discover PDS colums of other types
        other_psd_type_cols = filter(lambda c: any(
            [c.startswith(n) and not c.endswith(psd_suffix) for n in psd_cols]
        ), cols)

        df = df.drop(columns=other_psd_type_cols)
        df = df.rename(
            lambda c: c if not c.endswith(psd_suffix) \
                else c.split(psd_suffix)[0],
            axis='columns'
        )
        return df

    df_control = df_control.assign(T=0)
    df_treated = df_treated.assign(T=1)

    return pd.concat([consolidate_psd(df_control),
                      consolidate_psd(df_treated)])


def mark_dataset(
        df: pd.DataFrame,
        outcomes=('Y',),
        treatment='T',
        ignore=(),
        covariates_prefix="X_",
        outcomes_prefix="Y_",
) -> pd.DataFrame:
    """
    Marks dataset variables as covariates, treatment and outcome variables.
    @param df: DataFrame containing the dataset to split.
    @param outcomes: Names of columns with outcome variables.
    @param treatment: Name of column with treatment parameter.
    @param ignore: Covariate names to remove.
    @param covariates_prefix: Prefix to give to covariate variables in the
    output.
    @param outcomes_prefix: Prefix to give to outcome variables in the
    output.
    @return: DataFrame with marked covariates, outcomes and treatment in
    this order and marked according to the prefixes.
    """

    treatment = [treatment]

    # Remove ignored columns
    df = df.drop(columns=ignore, inplace=False)

    # Rename columns
    df_T = df[treatment]

    df_Y = df[outcomes]
    outcomes_map = {o: f'{outcomes_prefix}{o}' for o in outcomes}
    df_Y = df_Y.rename(columns=outcomes_map, inplace=False)

    covariates = [c for c in df.columns
                  if c not in treatment and c not in outcomes]
    df_X = df[covariates]
    covariates_map = {c: f'{covariates_prefix}{c}' for c in covariates}
    df_X = df_X.rename(columns=covariates_map, inplace=False)

    # Reorder columns
    return pd.concat([df_X, df_Y, df_T], axis=1)


def split_dataset(
        df: pd.DataFrame,
        covariates_prefix="X_",
        outcomes_prefix="Y_",
        treatment='T',
        scale_covariates=False,
) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """

    @param df: DataFrame to split.
    @param covariates_prefix: Prefix of the covariate variables in the
    input.
    @param outcomes_prefix: Prefix of the outcome variables in the
    input.
    @param treatment: Name of treatment variable.
    @param scale_covariates: Whether to scale the covariates using a
    standard scaler.
    @return: Tuple (X, y, y) containing the covariates, outcomes and
    treatment respectively.
    """
    covariates = [c for c in df.columns if c.startswith(covariates_prefix)]
    outcomes = [c for c in df.columns if c.startswith(outcomes_prefix)]

    X = df[covariates].values
    y = df[outcomes].values
    t = df[treatment].values.reshape(-1)

    if y.shape[1] == 1:
        y = y.reshape(-1)

    if scale_covariates:
        X = StandardScaler().fit_transform(X)

    return X, y, t
